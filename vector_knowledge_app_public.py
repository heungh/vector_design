import streamlit as st
import boto3
import os
import yaml
import re
import json
from datetime import datetime
import asyncio
from typing import Dict, List, Optional, Tuple
import hashlib
from collections import Counter
import time
from concurrent.futures import ThreadPoolExecutor
import logging

# ì„¤ì •
KNOWLEDGE_BASE_ID = ""  # ì…ë ¥í•„ìš”
S3_BUCKET_NAME = ""  # ì…ë ¥í•„ìš”
VECTOR_DIR = "vector"
SIMILARITY_THRESHOLD = 70
MAX_RETRIES = 3
BATCH_SIZE = 10

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class EnhancedVectorKnowledgeApp:
    def __init__(self):
        self.bedrock_agent_client = boto3.client(
            "bedrock-agent-runtime", region_name="us-east-1"
        )
        self.bedrock_runtime_client = boto3.client(
            "bedrock-runtime", region_name="us-east-1"
        )
        # S3 í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ê°œì„ 
        try:
            self.s3_client = boto3.client("s3", region_name="us-east-1")
            # S3 ì—°ê²° í…ŒìŠ¤íŠ¸
            self.s3_client.head_bucket(Bucket=S3_BUCKET_NAME)
            logger.info(f"S3 ë²„í‚· {S3_BUCKET_NAME} ì—°ê²° ì„±ê³µ")
        except Exception as e:
            logger.error(f"S3 ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            self.s3_client = None

        os.makedirs(VECTOR_DIR, exist_ok=True)
        self._file_cache = {}

    def _normalize_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ê·œí™”"""
        return re.sub(r"[^\w\s]", "", text.lower().strip())

    def _extract_keywords(self, content: str) -> List[str]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ ë° ê¸°ë³¸ íƒœê·¸ ì¶”ê°€"""
        normalized = self._normalize_text(content)
        words = normalized.split()

        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = {
            "the",
            "a",
            "an",
            "and",
            "or",
            "but",
            "in",
            "on",
            "at",
            "to",
            "for",
            "of",
            "with",
            "by",
        }
        words = [w for w in words if w not in stopwords and len(w) > 2]

        # ë¹ˆë„ ê¸°ë°˜ í‚¤ì›Œë“œ ì„ ë³„
        word_freq = Counter(words)
        keywords = [word for word, freq in word_freq.most_common(10)]

        # ê¸°ë³¸ íƒœê·¸ ì¶”ê°€
        content_lower = content.lower()
        if any(
            term in content_lower
            for term in ["database", "db", "sql", "mysql", "postgresql"]
        ):
            keywords.append("database")
        if any(
            term in content_lower
            for term in ["performance", "optimization", "tuning", "speed"]
        ):
            keywords.append("performance")
        if any(
            term in content_lower
            for term in ["error", "troubleshoot", "debug", "fix", "issue"]
        ):
            keywords.append("troubleshooting")

        return list(set(keywords))

    def _check_content_similarity(self, new_content: str) -> Tuple[bool, Dict]:
        """ì½˜í…ì¸  ìœ ì‚¬ë„ ê²€ì‚¬"""
        if not os.path.exists(VECTOR_DIR):
            return False, {}

        new_keywords = set(self._normalize_text(new_content).split())
        max_similarity = 0
        similar_file = None

        for filename in os.listdir(VECTOR_DIR):
            if not filename.endswith(".md"):
                continue

            filepath = os.path.join(VECTOR_DIR, filename)

            # íŒŒì¼ ìºì‹±
            if filepath not in self._file_cache:
                try:
                    with open(filepath, "r", encoding="utf-8") as f:
                        self._file_cache[filepath] = f.read()
                except Exception as e:
                    logger.error(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ {filepath}: {e}")
                    continue

            existing_content = self._file_cache[filepath]
            existing_keywords = set(self._normalize_text(existing_content).split())

            # ìœ ì‚¬ë„ ê³„ì‚°
            if existing_keywords:
                intersection = len(new_keywords & existing_keywords)
                union = len(new_keywords | existing_keywords)
                similarity = (intersection / union) * 100 if union > 0 else 0

                if similarity > max_similarity:
                    max_similarity = similarity
                    similar_file = filename

        is_similar = max_similarity >= SIMILARITY_THRESHOLD
        return is_similar, {
            "max_similarity": max_similarity,
            "similar_file": similar_file,
            "threshold": SIMILARITY_THRESHOLD,
        }

    async def _analyze_content_conflicts(
        self, new_content: str, existing_content: str
    ) -> Tuple[bool, str]:
        """Claudeë¥¼ ì‚¬ìš©í•œ ì½˜í…ì¸  ì¶©ëŒ ë¶„ì„"""
        prompt = f"""
ë‹¤ìŒ ë‘ ì½˜í…ì¸ ë¥¼ ë¹„êµí•˜ì—¬ ì‹¤ì œ ì¶©ëŒì´ë‚˜ ëª¨ìˆœì´ ìˆëŠ”ì§€ ë¶„ì„í•´ì£¼ì„¸ìš”:

ê¸°ì¡´ ì½˜í…ì¸ :
{existing_content[:1000]}

ìƒˆ ì½˜í…ì¸ :
{new_content[:1000]}

ë¶„ì„ ê²°ê³¼ë¥¼ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
CONFLICT: [YES/NO]
REASON: [ì¶©ëŒ ì´ìœ  ë˜ëŠ” ë³´ì™„ ê´€ê³„ ì„¤ëª…]
"""

        try:
            response = self.bedrock_runtime_client.invoke_model(
                modelId="anthropic.claude-3-sonnet-20240229-v1:0",
                body=json.dumps(
                    {
                        "anthropic_version": "bedrock-2023-05-31",
                        "max_tokens": 500,
                        "messages": [{"role": "user", "content": prompt}],
                    }
                ),
            )

            result = json.loads(response["body"].read())
            analysis = result["content"][0]["text"]

            has_conflict = "CONFLICT: YES" in analysis
            return has_conflict, analysis

        except Exception as e:
            logger.error(f"ì¶©ëŒ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return False, "ë¶„ì„ ì‹¤íŒ¨"

    def _retry_operation(self, operation, *args, **kwargs):
        """ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜"""
        for attempt in range(MAX_RETRIES):
            try:
                return operation(*args, **kwargs)
            except Exception as e:
                if attempt == MAX_RETRIES - 1:
                    raise e
                logger.warning(f"ì¬ì‹œë„ {attempt + 1}/{MAX_RETRIES}: {e}")
                time.sleep(2**attempt)

    def _batch_s3_upload(self, files: List[Tuple[str, str]]) -> List[bool]:
        """ë°°ì¹˜ S3 ì—…ë¡œë“œ"""
        results = []

        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = []
            for local_path, s3_key in files:
                future = executor.submit(self._upload_to_s3, local_path, s3_key)
                futures.append(future)

            for future in futures:
                try:
                    result = future.result(timeout=30)
                    results.append(result)
                except Exception as e:
                    logger.error(f"ë°°ì¹˜ ì—…ë¡œë“œ ì˜¤ë¥˜: {e}")
                    results.append(False)

        return results

    def _upload_to_s3(self, local_path: str, s3_key: str) -> bool:
        """S3 ì—…ë¡œë“œ with ì¬ì‹œë„"""
        if not self.s3_client:
            logger.error("S3 í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
            return False

        try:
            logger.info(
                f"S3 ì—…ë¡œë“œ ì‹œë„: {local_path} -> s3://{S3_BUCKET_NAME}/{s3_key}"
            )

            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not os.path.exists(local_path):
                logger.error(f"ë¡œì»¬ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {local_path}")
                return False

            self._retry_operation(
                self.s3_client.upload_file, local_path, S3_BUCKET_NAME, s3_key
            )
            logger.info(f"S3 ì—…ë¡œë“œ ì„±ê³µ: {s3_key}")
            return True
        except Exception as e:
            logger.error(f"S3 ì—…ë¡œë“œ ì‹¤íŒ¨ {s3_key}: {e}")
            return False

    async def save_to_vector_store(
        self,
        content: str,
        topic: str,
        category: str = "examples",
        tags: List[str] = None,
        force_save: bool = False,
    ) -> Dict:
        """í–¥ìƒëœ ë²¡í„° ì €ì¥ì†Œ ì €ì¥"""
        try:
            # 1. force_saveê°€ ì•„ë‹Œ ê²½ìš° ì¤‘ë³µ/ì¶©ëŒ ê²€ì‚¬
            if not force_save:
                is_similar, similarity_info = self._check_content_similarity(content)

                if is_similar:
                    # ìœ ì‚¬í•œ íŒŒì¼ì´ ìˆìœ¼ë©´ ì¶©ëŒ ë¶„ì„
                    similar_filepath = os.path.join(
                        VECTOR_DIR, similarity_info["similar_file"]
                    )
                    with open(similar_filepath, "r", encoding="utf-8") as f:
                        existing_content = f.read()

                    has_conflict, analysis = await self._analyze_content_conflicts(
                        content, existing_content
                    )

                    if has_conflict:
                        return {
                            "success": False,
                            "message": "ì½˜í…ì¸  ì¶©ëŒ ê°ì§€",
                            "similarity_info": similarity_info,
                            "conflict_analysis": analysis,
                        }

            # 2. í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë©”íƒ€ë°ì´í„° ìƒì„±
            keywords = self._extract_keywords(content)
            if tags:
                keywords.extend(tags)
            keywords = list(set(keywords))

            # 3. íŒŒì¼ ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{timestamp}_{topic}.md"
            filepath = os.path.join(VECTOR_DIR, filename)

            metadata = {
                "title": topic,
                "category": category,
                "tags": keywords,
                "version": 1,
                "last_updated": datetime.now().isoformat(),
                "author": "DB Assistant",
                "source": "conversation",
                "similarity_check": similarity_info if not force_save else None,
            }

            # YAML í—¤ë” + ì½˜í…ì¸ 
            file_content = (
                f"---\n{yaml.dump(metadata, default_flow_style=False)}---\n\n{content}"
            )

            with open(filepath, "w", encoding="utf-8") as f:
                f.write(file_content)

            # 4. S3 ì—…ë¡œë“œ (ë°°ì¹˜ ì²˜ë¦¬)
            s3_key = f"{category}/{filename}"
            upload_success = self._upload_to_s3(filepath, s3_key)

            return {
                "success": True,
                "message": "ì €ì¥ ì™„ë£Œ",
                "filename": filename,
                "filepath": filepath,
                "s3_uploaded": upload_success,
                "metadata": metadata,
            }

        except Exception as e:
            logger.error(f"ì €ì¥ ì˜¤ë¥˜: {e}")
            return {"success": False, "message": f"ì €ì¥ ì‹¤íŒ¨: {str(e)}"}

    def update_vector_content(
        self, filename: str, new_content: str, update_mode: str = "append"
    ) -> Dict:
        """ì½˜í…ì¸  ì—…ë°ì´íŠ¸"""
        filepath = os.path.join(VECTOR_DIR, filename)

        if not os.path.exists(filepath):
            return {"success": False, "message": "íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}

        try:
            # ê¸°ì¡´ íŒŒì¼ ì½ê¸°
            with open(filepath, "r", encoding="utf-8") as f:
                content = f.read()

            # YAML í—¤ë” ë¶„ë¦¬
            if content.startswith("---"):
                parts = content.split("---", 2)
                if len(parts) >= 3:
                    yaml_header = parts[1]
                    existing_content = parts[2].strip()
                    metadata = yaml.safe_load(yaml_header)
                else:
                    metadata = {}
                    existing_content = content
            else:
                metadata = {}
                existing_content = content

            # ì½˜í…ì¸  ì—…ë°ì´íŠ¸
            if update_mode == "append":
                updated_content = existing_content + "\n\n" + new_content
            else:  # replace
                updated_content = new_content

            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            metadata["last_updated"] = datetime.now().isoformat()
            metadata["version"] = metadata.get("version", 1) + 1

            # íŒŒì¼ ì €ì¥
            file_content = f"---\n{yaml.dump(metadata, default_flow_style=False)}---\n\n{updated_content}"

            with open(filepath, "w", encoding="utf-8") as f:
                f.write(file_content)

            # S3 ì—…ë¡œë“œ
            category = metadata.get("category", "examples")
            s3_key = f"{category}/{filename}"
            upload_success = self._upload_to_s3(filepath, s3_key)

            return {
                "success": True,
                "message": "ì—…ë°ì´íŠ¸ ì™„ë£Œ",
                "filename": filename,
                "s3_uploaded": upload_success,
                "metadata": metadata,
            }

        except Exception as e:
            logger.error(f"ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
            return {"success": False, "message": f"ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}"}

    def query_vector_store(self, query: str, max_results: int = 5) -> str:
        """í–¥ìƒëœ ë²¡í„° ê²€ìƒ‰"""
        try:
            response = self._retry_operation(
                self.bedrock_agent_client.retrieve,
                knowledgeBaseId=KNOWLEDGE_BASE_ID,
                retrievalQuery={"text": query},
                retrievalConfiguration={
                    "vectorSearchConfiguration": {"numberOfResults": max_results}
                },
            )

            if not response.get("retrievalResults"):
                return "ğŸ” ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

            results = []
            for i, result in enumerate(response["retrievalResults"], 1):
                content = result["content"]["text"]
                score = result.get("score", 0)
                metadata = result.get("metadata", {})
                source = metadata.get("x-amz-bedrock-kb-source-uri", "ì•Œ ìˆ˜ ì—†ìŒ")

                results.append(
                    f"""
**ê²°ê³¼ {i}** (ì ìˆ˜: {score:.3f})
ğŸ“„ ì¶œì²˜: {source}
ğŸ“ ë‚´ìš©: {content[:500]}{'...' if len(content) > 500 else ''}
---
"""
                )
            return "\n".join(results)

        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}"

    def sync_knowledge_base(self) -> Dict:
        """Knowledge Base ë™ê¸°í™”"""
        try:
            bedrock_agent = boto3.client("bedrock-agent", region_name="us-east-1")

            # ë°ì´í„° ì†ŒìŠ¤ ì¡°íšŒ
            kb_response = bedrock_agent.get_knowledge_base(
                knowledgeBaseId=KNOWLEDGE_BASE_ID
            )
            data_source_id = None

            # ì²« ë²ˆì§¸ ë°ì´í„° ì†ŒìŠ¤ ì‚¬ìš©
            ds_response = bedrock_agent.list_data_sources(
                knowledgeBaseId=KNOWLEDGE_BASE_ID
            )
            if ds_response.get("dataSourceSummaries"):
                data_source_id = ds_response["dataSourceSummaries"][0]["dataSourceId"]

            if not data_source_id:
                return {"success": False, "message": "ë°ì´í„° ì†ŒìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}

            # ë™ê¸°í™” ì‘ì—… ì‹œì‘
            sync_response = bedrock_agent.start_ingestion_job(
                knowledgeBaseId=KNOWLEDGE_BASE_ID, dataSourceId=data_source_id
            )

            job_id = sync_response["ingestionJob"]["ingestionJobId"]
            status = sync_response["ingestionJob"]["status"]

            return {
                "success": True,
                "message": f"ë™ê¸°í™” ì‹œì‘ë¨ (Job ID: {job_id})",
                "job_id": job_id,
                "status": status,
            }

        except Exception as e:
            logger.error(f"ë™ê¸°í™” ì˜¤ë¥˜: {e}")
            return {"success": False, "message": f"ë™ê¸°í™” ì‹¤íŒ¨: {str(e)}"}

    async def chat_with_claude(
        self,
        message: str,
        include_vector_search: bool = True,
        chat_history: List = None,
    ) -> Tuple[str, List]:
        """Claudeì™€ ì±„íŒ… (ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ í¬í•¨)"""
        try:
            context = ""
            if include_vector_search:
                search_results = self.query_vector_store(message, max_results=3)
                context = f"\n\nì°¸ê³  ìë£Œ:\n{search_results}\n\n"

            # ì±„íŒ… íˆìŠ¤í† ë¦¬ êµ¬ì„±
            messages = []
            if chat_history:
                messages.extend(chat_history)

            user_message = f"{context}ì§ˆë¬¸: {message}"
            messages.append({"role": "user", "content": user_message})

            # Claude 4 ì‹œë„
            try:
                response = self.bedrock_runtime_client.invoke_model(
                    modelId="anthropic.claude-3-5-sonnet-20241022-v2:0",
                    body=json.dumps(
                        {
                            "anthropic_version": "bedrock-2023-05-31",
                            "max_tokens": 2000,
                            "messages": messages,
                        }
                    ),
                )
            except:
                # Fallback to Claude 3.7
                response = self.bedrock_runtime_client.invoke_model(
                    modelId="anthropic.claude-3-sonnet-20240229-v1:0",
                    body=json.dumps(
                        {
                            "anthropic_version": "bedrock-2023-05-31",
                            "max_tokens": 2000,
                            "messages": messages,
                        }
                    ),
                )

            result = json.loads(response["body"].read())
            assistant_response = result["content"][0]["text"]

            # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
            messages.append({"role": "assistant", "content": assistant_response})

            return assistant_response, messages

        except Exception as e:
            logger.error(f"ì±„íŒ… ì˜¤ë¥˜: {e}")
            return f"âŒ ì±„íŒ… ì‹¤íŒ¨: {str(e)}", chat_history or []


def main():
    st.set_page_config(
        page_title="Enhanced Vector Knowledge Base", page_icon="ğŸ§ ", layout="wide"
    )

    st.title("ğŸ§  Enhanced Vector Knowledge Base with AI Chat")
    st.markdown("í–¥ìƒëœ ë²¡í„° ê²€ìƒ‰, AI ì±„íŒ…, ìŠ¤ë§ˆíŠ¸ ì—…ë¡œë“œ ê¸°ëŠ¥")

    app = EnhancedVectorKnowledgeApp()

    # ì‚¬ì´ë“œë°” ì„¤ì •
    st.sidebar.title("âš™ï¸ ì„¤ì •")
    mode = st.sidebar.selectbox(
        "ëª¨ë“œ ì„ íƒ", ["ğŸ” ê²€ìƒ‰", "ğŸ’¬ AI ì±„íŒ…", "ğŸ“¤ ì—…ë¡œë“œ", "ğŸ”„ ë™ê¸°í™”"]
    )

    if mode == "ğŸ” ê²€ìƒ‰":
        st.header("ğŸ” ë²¡í„° ê²€ìƒ‰")

        col1, col2 = st.columns([3, 1])
        with col1:
            query = st.text_input(
                "ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”:", placeholder="ì˜ˆ: ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ìµœì í™”"
            )
        with col2:
            max_results = st.number_input(
                "ìµœëŒ€ ê²°ê³¼ ìˆ˜", min_value=1, max_value=20, value=5
            )

        if st.button("ğŸ” ê²€ìƒ‰", type="primary"):
            if query:
                with st.spinner("ê²€ìƒ‰ ì¤‘..."):
                    results = app.query_vector_store(query, max_results)
                st.markdown("### ê²€ìƒ‰ ê²°ê³¼")
                st.markdown(results)
            else:
                st.warning("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    elif mode == "ğŸ’¬ AI ì±„íŒ…":
        st.header("ğŸ’¬ AI ì±„íŒ…")

        # ì±„íŒ… íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
        if "chat_history" not in st.session_state:
            st.session_state.chat_history = []

        # ì„¤ì •
        include_search = st.checkbox("ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ í¬í•¨", value=True)

        # ì±„íŒ… íˆìŠ¤í† ë¦¬ í‘œì‹œ
        for i, msg in enumerate(st.session_state.chat_history):
            if msg["role"] == "user":
                st.chat_message("user").write(msg["content"])
            else:
                st.chat_message("assistant").write(msg["content"])

        # ìƒˆ ë©”ì‹œì§€ ì…ë ¥
        if prompt := st.chat_input("Claudeì—ê²Œ ì§ˆë¬¸í•˜ì„¸ìš”..."):
            st.chat_message("user").write(prompt)

            with st.spinner("Claudeê°€ ë‹µë³€ ì¤‘..."):
                response, updated_history = asyncio.run(
                    app.chat_with_claude(
                        prompt, include_search, st.session_state.chat_history
                    )
                )

            st.chat_message("assistant").write(response)
            st.session_state.chat_history = updated_history

        # íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™” ë²„íŠ¼
        if st.button("ğŸ—‘ï¸ ì±„íŒ… íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"):
            st.session_state.chat_history = []
            st.rerun()

    elif mode == "ğŸ“¤ ì—…ë¡œë“œ":
        st.header("ğŸ“¤ ìŠ¤ë§ˆíŠ¸ ë¬¸ì„œ ì—…ë¡œë“œ")

        # ì—…ë¡œë“œ ë°©ì‹ ì„ íƒ
        upload_type = st.radio("ì—…ë¡œë“œ ë°©ì‹", ["ğŸ“ íŒŒì¼ ì—…ë¡œë“œ", "âœï¸ ì§ì ‘ ì…ë ¥"])

        content = ""
        if upload_type == "ğŸ“ íŒŒì¼ ì—…ë¡œë“œ":
            uploaded_file = st.file_uploader("í…ìŠ¤íŠ¸ íŒŒì¼ ì„ íƒ", type=["txt", "md"])
            if uploaded_file:
                content = uploaded_file.read().decode("utf-8")
                st.text_area("íŒŒì¼ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°", content, height=200, disabled=True)
        else:
            content = st.text_area("ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”:", height=300)

        if content:
            col1, col2 = st.columns(2)
            with col1:
                topic = st.text_input("ì£¼ì œ (10ì ì´ë‚´)", max_chars=10)
                category = st.selectbox(
                    "ì¹´í…Œê³ ë¦¬",
                    [
                        "database-standards",
                        "performance-optimization",
                        "troubleshooting",
                        "examples",
                    ],
                )
            with col2:
                tags_input = st.text_input(
                    "íƒœê·¸ (ì‰¼í‘œë¡œ êµ¬ë¶„)", placeholder="sql, optimization, mysql"
                )
                force_save = st.checkbox(
                    "ì¤‘ë³µ ê²€ì‚¬ ê±´ë„ˆë›°ê¸°", help="ìœ ì‚¬í•œ ë‚´ìš©ì´ ìˆì–´ë„ ê°•ì œë¡œ ì €ì¥"
                )

            tags = (
                [tag.strip() for tag in tags_input.split(",") if tag.strip()]
                if tags_input
                else []
            )

            if st.button("ğŸ’¾ ì €ì¥", type="primary"):
                if topic:
                    with st.spinner("ì €ì¥ ì¤‘... (ìœ ì‚¬ë„ ê²€ì‚¬ ë° ì¶©ëŒ ë¶„ì„ í¬í•¨)"):
                        result = asyncio.run(
                            app.save_to_vector_store(
                                content, topic, category, tags, force_save
                            )
                        )

                    if result["success"]:
                        st.success(f"âœ… {result['message']}")
                        st.json(result["metadata"])
                        if not result.get("s3_uploaded"):
                            st.warning("âš ï¸ S3 ì—…ë¡œë“œ ì‹¤íŒ¨ - ë¡œì»¬ì—ë§Œ ì €ì¥ë¨")
                    else:
                        st.error(f"âŒ {result['message']}")
                        if "similarity_info" in result:
                            st.warning(
                                f"ìœ ì‚¬í•œ íŒŒì¼: {result['similarity_info']['similar_file']} "
                                f"(ìœ ì‚¬ë„: {result['similarity_info']['max_similarity']:.1f}%)"
                            )
                        if "conflict_analysis" in result:
                            st.text_area(
                                "ì¶©ëŒ ë¶„ì„ ê²°ê³¼",
                                result["conflict_analysis"],
                                height=150,
                            )
                else:
                    st.warning("ì£¼ì œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    elif mode == "ğŸ”„ ë™ê¸°í™”":
        st.header("ğŸ”„ Knowledge Base ë™ê¸°í™”")
        st.markdown("S3ì˜ ìµœì‹  íŒŒì¼ë“¤ì„ Knowledge Baseì— ë™ê¸°í™”í•©ë‹ˆë‹¤.")

        if st.button("ğŸ”„ ë™ê¸°í™” ì‹œì‘", type="primary"):
            with st.spinner("ë™ê¸°í™” ì¤‘..."):
                result = app.sync_knowledge_base()

            if result["success"]:
                st.success(f"âœ… {result['message']}")
                st.info(f"ìƒíƒœ: {result.get('status', 'Unknown')}")
            else:
                st.error(f"âŒ {result['message']}")

    # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.markdown("---")
        st.markdown("### ğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ")

        # ìºì‹œ ìƒíƒœ
        cache_size = len(app._file_cache)
        st.metric("íŒŒì¼ ìºì‹œ", f"{cache_size}ê°œ")

        # ë¡œì»¬ íŒŒì¼ ìˆ˜
        if os.path.exists(VECTOR_DIR):
            local_files = len([f for f in os.listdir(VECTOR_DIR) if f.endswith(".md")])
            st.metric("ë¡œì»¬ íŒŒì¼", f"{local_files}ê°œ")

        if st.button("ğŸ—‘ï¸ ìºì‹œ ì´ˆê¸°í™”"):
            app._file_cache.clear()
            st.success("ìºì‹œê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
